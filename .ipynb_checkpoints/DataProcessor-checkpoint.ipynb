{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a32395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "from transformers import BartForConditionalGeneration,BartTokenizer\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", \n",
    "                      model=\"facebook/bart-large-cnn\",\n",
    "                      device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afbbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextSummary(context,max_len=5000):\n",
    "    return summarizer(context[:max_len], \n",
    "                      max_length=max_len//3 + 150, \n",
    "                      min_length=150, \n",
    "                      do_sample=False)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95e4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset_builder,load_dataset\n",
    "convAI_data = load_dataset_builder('multidoc2dial')\n",
    "\n",
    "train_data= load_dataset('multidoc2dial', split='train', ignore_verifications=True)\n",
    "test_data= load_dataset('multidoc2dial', split='test', \n",
    "                                            ignore_verifications=True)\n",
    "dev_data= load_dataset('multidoc2dial', split='validation', \n",
    "                                            ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7ba491",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[4]['context'].strip().split(\"\\n\\n\")[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7096da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from mosestokenizer import MosesDetokenizer,MosesSentenceSplitter\n",
    "import tqdm\n",
    "from nltk.util import ngrams\n",
    "import random\n",
    "splitter = MosesSentenceSplitter('en')\n",
    "detokenizer = MosesDetokenizer('en')\n",
    "def normalize_whitespace(string):\n",
    "    return re.sub(r\"(\\s)\\1{1,}\", r\"\\1\", string)\n",
    "def cleanDocument(document):\n",
    "  document = re.sub(r'\\[\\d+\\s?\\]','',document)\n",
    "  document = re.sub(r'(\\d\\.\\s+|[a-z]\\)\\s+|•\\s+|[A-Z]\\.\\s+|[IVX]+\\.\\s+)','',document)\n",
    "  document = normalize_whitespace(document.replace('\\n','')).strip()\n",
    "  return document\n",
    "def raw_checks(answer, context):\n",
    " dat= context.strip().split(\"\\n\\n\")\n",
    " if len(dat)==1:\n",
    "   dat = dat[0].strip().split(\"\\n\")\n",
    " contains = [s.strip() for s in dat if answer.strip() in s.strip()]\n",
    " return \" \".join(contains)\n",
    " \n",
    "def getOverlap(answer_sentence,answer_start,document_context,n=4):\n",
    "  \n",
    "  \n",
    "  doc_context =  raw_checks(answer_sentence,document_context)\n",
    "  #print(doc_context)\n",
    "  \n",
    "  document = cleanDocument(doc_context)\n",
    "  answer_sentence = cleanDocument(answer_sentence)\n",
    "  document_list = splitter(paragraph=[detokenizer(document.split())])\n",
    "  print(document_list)\n",
    "  #document_list = [normalize_whitespace(re.sub(r'^(\\d\\.\\s+|[a-z]\\)\\s+|•\\s+|[A-Z]\\.\\s+|[IVX]+\\.\\s+)','',s)) for s in document_list]\n",
    "  doc_ngrams =[detokenizer(' '.join(s).split()) for s in  list(ngrams(document_list, n))]\n",
    "\n",
    "\n",
    "  answer_sentence_ = normalize_whitespace(re.sub(r'\\[\\d+\\]','',answer_sentence.replace('\\n',''))).strip()\n",
    "  \n",
    "\n",
    "  #try:\n",
    "  answer = detokenizer(' '.join(splitter(paragraph=[detokenizer(answer_sentence_.split())])).split())\n",
    "  #except Exception as e:\n",
    "  #  print(e)\n",
    "  #  answer = answer_sentence_\n",
    "  answer_chunk = [s for s in doc_ngrams if answer in s or answer_sentence_ in s]\n",
    "  print(len(answer_chunk))\n",
    "  answer_chunk = random.choice(answer_chunk) \n",
    "  #if len(answer_chunk)>0 else \"\"\n",
    "  #print(f\"{answer_sentence_} <<<found in>>>>>> \\n\"+answer_chunk)\n",
    "  if len(answer_chunk)==0:\n",
    "    print(\"No context found\")\n",
    "    # generate a sunmmary of the article then post-pend the answer_sentence\n",
    "    answer_chunk = getContextSummary(document,len(document)) + answer_sentence_\n",
    "  return answer_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74808c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2af8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[idk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e4bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_= answer_start[0]+ len(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83181d",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer,len(answer[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12be867",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanDocument(train_data[idk]['context'][answer_start[0]-600:end_+800] ).split('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7659fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk=646\n",
    "context=train_data[idk]['context']\n",
    "answer = train_data[idk]['answers'][\"text\"]\n",
    "answer_start = train_data[idk][\"answers\"]['answer_start']\n",
    "question = train_data[idk]['question']\n",
    "ac=getOverlap(answer[0],answer_start,str(context),random.choice([3,4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74250a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa8f975",
   "metadata": {},
   "outputs": [],
   "source": [
    "[f for f in train_data[1]['context'].split(\"\\n\\n\") if \"Sign up or log into MyDMV\" in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc1245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9410f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_present = []\n",
    "for idx in tqdm.tqdm(range(0,len(train_data))):\n",
    "    dat= train_data[idx]\n",
    "    document_context = dat['context']\n",
    "    question, history= dat['question'].split('[SEP]')\n",
    "    answer_text = dat['answers']['text'][0]\n",
    "    answer_ids = dat['answers']['answer_start'][0]\n",
    "    \n",
    "    if dat['answers']['text'][0] not in document_context:\n",
    "        not_present.append(idx)\n",
    "    else:\n",
    "        doc_context =  raw_checks(answer_text,document_context)\n",
    "        document = cleanDocument(doc_context)\n",
    "        answer_context = cleanDocument(answer_text)\n",
    "        \n",
    "        print(idx,document,'\\n',answer_context)\n",
    "        \n",
    "        assert answer_context in document, \"Error: \"\n",
    "        #document_list = splitter(paragraph=[detokenizer(document.split())])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b11463",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_context =  raw_checks(answer_text,document_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06bf835",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_context , doc_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e3d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk= 0\n",
    "\n",
    "for idk in range(638,650):\n",
    "    context=train_data[idk]['context']\n",
    "    answer = train_data[idk]['answers'][\"text\"]\n",
    "    question = train_data[idk]['question']\n",
    "    print(idk)\n",
    "    getOverlap(answer[0],context,random.choice([7,6,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa83f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the dataset to coallate all the data\n",
    "def processDoc2ChatData(datapack,verbose=False):\n",
    "    datapack_strip = datapack#.values\n",
    "    dataset = []\n",
    "    for idx in tqdm.tqdm(range(len(datapack_strip))) if verbose else enumerate(range(len(datapack_strip))):\n",
    "        #print(idx)\n",
    "        dat= datapack_strip[idx]\n",
    "        document_context = dat['context']\n",
    "        question, history= dat['question'].split('[SEP]')\n",
    "        answer_text = dat['answers']['text'][0]\n",
    "        answer_ids = dat['answers']['answer_start'][0]\n",
    "\n",
    "        answer_fact = getOverlap(answer_text,document_context,n=random.choice([7,8]))\n",
    "        \n",
    "        data = dict(document=document_context, \n",
    "                    question=question,\n",
    "                    history=history,\n",
    "                    org_answer_text= answer_text,\n",
    "                    answer_start= answer_ids,\n",
    "                    retrieved_fact=answer_fact\n",
    "                    )\n",
    "        dataset.append(data)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f006cc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_train=processDoc2ChatData(train_data,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03989f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "idk= 3\n",
    "context=cleanDocument(train_data[idk]['context'])[:]\n",
    "getContextSummary(context, len(context))\n",
    "#max_length=3000, min_length=130, do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71977a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de7d60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-mpnet-base-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = torch.FloatTensor(model.encode([\"What is LNG\"],normalize_embeddings=False))\n",
    "e2 = torch.FloatTensor(model.encode([\"What is LNG\"],normalize_embeddings=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26e93b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3256551",
   "metadata": {},
   "outputs": [],
   "source": [
    "e1[0,:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68459a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "e2[0,:10].numpy()==e1[0,:10].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d78063f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.all(e1!=e2):\n",
    "    print(6464)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "651a0ea9c99ff4fc9d0cad7e667d2a622e86969049e30a30403341f776f67719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
