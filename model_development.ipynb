{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import EncoderDecoderModel\n",
    "from torch.utils.data import Dataset\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from typing import Optional, Union, Callable, Dict, List, Tuple\n",
    "\n",
    "encoder_model_base = \"roberta-base\"\n",
    "decoder_model_base = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(encoder_model_base)\n",
    "special_tokens = [\"[FACTS]\",\"[RESPONSE]\",\"[CONV_HISTORY]\"]\n",
    "tokenizer.add_special_tokens({'additional_special_tokens': special_tokens})\n",
    "\n",
    "\n",
    "# Load the data\n",
    "with open(\"processed_data/test_data.json\",\"r\") as test_file:\n",
    "    test_data=json.load(test_file)\n",
    "    \n",
    "with open(\"processed_data/dev_data.json\",\"r\") as dev_file:\n",
    "    dev_data=json.load(dev_file)\n",
    "    \n",
    "with open(\"processed_data/train_data.json\",\"r\") as train_file:\n",
    "    train_data = json.load(train_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Features:\n",
    "    input_ids: List[int]\n",
    "    attention_mask: List[int]\n",
    "    labels: Optional[List[int]]\n",
    "    decoder_attention_mask: Optional[List[int]]\n",
    "class MultiDoc2ChatDataset(Dataset):\n",
    "    def __init__(self,tokenizer,data_pack) -> None:\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.data_pack= data_pack\n",
    "        self.change_data_mode(1)\n",
    "        super().__init__()\n",
    "        \n",
    "    def change_data_mode(self, mode=1):\n",
    "        self.mode = mode > 1\n",
    "        \n",
    "    \n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        return len(self.data_pack)\n",
    "    \n",
    "    def processExample(self,data):\n",
    "        \n",
    "        facts= data[\"facts\"]\n",
    "        utterance= data[\"utterance\"]\n",
    "        question = data[\"current_question\"]\n",
    "        conv_history = data[\"conv_history\"].replace(\"||\",\" [SEP] \")\n",
    "        \n",
    "        fact_question_passage = question +' [FACTS] '+facts\n",
    "        response_passage = '[CONV_HISTORY] '+conv_history + \" [RESPONSE] \"+ utterance\n",
    "        \n",
    "        # apply the tokenizer to convert the texts to the appropriate input\n",
    "        if not self.mode:\n",
    "            label_pack = self.tokenizer(response_passage, return_tensors=\"pt\")\n",
    "            label_seq = label_pack[\"input_ids\"].flatten()\n",
    "            label_attention = label_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        passage_pack = self.tokenizer(fact_question_passage, return_tensors=\"pt\")\n",
    "\n",
    "        passage_seq = passage_pack[\"input_ids\"].flatten()\n",
    "        passage_attention = passage_pack[\"attention_mask\"].flatten()\n",
    "\n",
    "        if not self.mode:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=label_seq,\n",
    "                decoder_attention_mask=label_attention,\n",
    "            )\n",
    "        else:\n",
    "            return Features(\n",
    "                input_ids=passage_seq,\n",
    "                attention_mask=passage_attention,\n",
    "                labels=[],\n",
    "                decoder_attention_mask=[],\n",
    "            )\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.processExample(self.data_pack[index])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultiDoc2ChatDataset(tokenizer,train_data)\n",
    "test_dataset = MultiDoc2ChatDataset(tokenizer,test_data)\n",
    "dev_dataset = MultiDoc2ChatDataset(tokenizer,dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "boolean = bool\n",
    "def pad_seq(\n",
    "    seq: List[np.ndarray], max_batch_len: int, pad_value: int, verbose=False\n",
    ") -> List[int]:\n",
    "    if len(seq) > max_batch_len:\n",
    "        seq = seq.to(torch.long).unsqueeze(0)[:, :max_batch_len]\n",
    "        return seq\n",
    "    pads = torch.from_numpy(np.array([pad_value] * (max_batch_len - len(seq))))\n",
    "    out = torch.concat([seq, pads], -1).to(torch.long).unsqueeze(0)\n",
    "    return out\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SmartCollator:\n",
    "    pad_token_id: int\n",
    "    label_pad_token_id: int = -100\n",
    "    is_gpt: boolean = False\n",
    "    max_len: int = 512\n",
    "    is_inference: boolean = False\n",
    "\n",
    "    def __call__(self, batch: List[Features]) -> Dict[str, torch.Tensor]:\n",
    "        batch_inputs: List = list()\n",
    "        batch_attention_masks: List = list()\n",
    "        decoder_attention_mask: List = list()\n",
    "        labels: List = list()\n",
    "        max_size = min([max([len(ex.input_ids) for ex in batch]), self.max_len])\n",
    "\n",
    "        max_size_output = min(\n",
    "            [max([len(ex.labels) for ex in batch]), self.max_len]\n",
    "        )  # type: ignore\n",
    "\n",
    "        for item in batch:\n",
    "            batch_inputs += [pad_seq(item.input_ids, max_size, self.pad_token_id)]\n",
    "            batch_attention_masks += [pad_seq(item.attention_mask, max_size, 0)]\n",
    "\n",
    "            if not self.is_gpt and not self.is_inference:\n",
    "                decoder_attention_mask += [\n",
    "                    pad_seq(item.decoder_attention_mask, max_size_output, 0)\n",
    "                ]\n",
    "            if not self.is_inference:\n",
    "                labels += [\n",
    "                    pad_seq(item.labels, max_size_output, self.label_pad_token_id)\n",
    "                ]\n",
    "        if not self.is_gpt:\n",
    "            if not self.is_inference:\n",
    "                return dict(\n",
    "                    input_ids=torch.concat(batch_inputs, 0),\n",
    "                    attention_mask=torch.concat(batch_attention_masks, 0),\n",
    "                    labels=torch.concat(labels, 0),\n",
    "                    decoder_attention_mask=torch.concat(decoder_attention_mask, 0),\n",
    "                )\n",
    "            else:\n",
    "                return dict(\n",
    "                    input_ids=torch.concat(batch_inputs, 0),\n",
    "                    attention_mask=torch.concat(batch_attention_masks, 0),\n",
    "                )\n",
    "        else:\n",
    "            return dict(\n",
    "                input_ids=torch.concat(batch_inputs, 0),\n",
    "                attention_mask=torch.concat(batch_attention_masks, 0),\n",
    "                labels=torch.concat(labels, 0),\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML,display_html\n",
    "HTML('<img height=\"400px\" width=\"900px\"  src =\"MicrosoftTeams-image (3).png\"/>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(device =  torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') ):\n",
    "    generator = EncoderDecoderModel.from_encoder_decoder_pretrained(encoder_model_base,\n",
    "                                                                    decoder_model_base, )\n",
    "    # update the tokens \n",
    "    generator.encoder.resize_token_embeddings(len(tokenizer))\n",
    "    generator.decoder.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # set the decoder start and end tokens\n",
    "    #generator.config.decoder_start_token_id = tokenizer.bos_token_id   \n",
    "    generator.config.decoder_start_token_id  = tokenizer.bos_token_id                                           \n",
    "    generator.config.eos_token_id = tokenizer.eos_token_id\n",
    "    generator.config.vocab_size = generator.config.encoder.vocab_size \n",
    "    generator.config.pad_token_id = tokenizer.pad_token_id\n",
    "    print(f\"\\n\\nNum Params: {generator.num_parameters()}\")\n",
    "    \n",
    "    return generator.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    PreTrainedModel,\n",
    "    DataCollator,\n",
    "    PreTrainedTokenizerBase,\n",
    "    EvalPrediction,\n",
    "    TrainerCallback,\n",
    ")\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RunArguments:\n",
    "\n",
    "    output_dir: str\n",
    "\n",
    "    learning_rate: float = 5e-5\n",
    "    run_id: str = \"\"\n",
    "    evaluation_strategy: str = \"epoch\"\n",
    "    save_strategy: str = \"epoch\"\n",
    "    max_seq_len: int = 600\n",
    "    warmup_ratio: float = 0.25\n",
    "    eval_steps: int = 1000\n",
    "    per_device_train_batch_size: int = 8\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    save_total_limit: int = 1\n",
    "    num_train_epochs: int = 5\n",
    "    weight_decay: float = 0.3\n",
    "    verbose: bool = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_trainer_arguments(args):\n",
    "    return TrainingArguments(\n",
    "        overwrite_output_dir=True,\n",
    "        adafactor=False,\n",
    "        load_best_model_at_end=True,\n",
    "        output_dir=args.output_dir + \"/\" + args.run_id + \"/\",\n",
    "        evaluation_strategy=args.evaluation_strategy,  # \"epoch\",\n",
    "        save_strategy=args.save_strategy,  # 'epoch',\n",
    "        lr_scheduler_type=args.lr_scheduler_type,\n",
    "        learning_rate=args.learning_rate,\n",
    "        save_total_limit=args.save_total_limit,\n",
    "        weight_decay=args.weight_decay,\n",
    "        warmup_ratio=args.warmup_ratio,\n",
    "        num_train_epochs=args.num_train_epochs,\n",
    "        per_device_train_batch_size=args.per_device_train_batch_size,\n",
    "        per_device_eval_batch_size=args.per_device_train_batch_size,\n",
    "        disable_tqdm=not args.verbose,\n",
    "        eval_steps=args.eval_steps,\n",
    "        save_steps=args.eval_steps,\n",
    "    )\n",
    "\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        device=torch.device(\"cuda\")\n",
    "        if torch.cuda.is_available()\n",
    "        else torch.device(\"cpu\"),\n",
    "        model: Union[PreTrainedModel, nn.Module] = None,  # type: ignore\n",
    "        args: TrainingArguments = None,  # type: ignore\n",
    "        data_collator: Optional[DataCollator] = None,\n",
    "        train_dataset: Optional[Dataset] = None,\n",
    "        eval_dataset: Optional[Dataset] = None,\n",
    "        tokenizer: Optional[PreTrainedTokenizerBase] = None,\n",
    "        model_init: Callable[[], PreTrainedModel] = None,\n",
    "        compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,\n",
    "        callbacks: Optional[List[TrainerCallback]] = None,\n",
    "        optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (\n",
    "            None,\n",
    "            None,\n",
    "        ),\n",
    "        preprocess_logits_for_metrics: Callable[\n",
    "            [torch.Tensor, torch.Tensor], torch.Tensor\n",
    "        ] = None,  # type: ignore\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model,\n",
    "            args,\n",
    "            data_collator,\n",
    "            train_dataset,\n",
    "            eval_dataset,\n",
    "            tokenizer,\n",
    "            model_init,\n",
    "            compute_metrics,\n",
    "            callbacks,\n",
    "            optimizers,\n",
    "            preprocess_logits_for_metrics,\n",
    "        )\n",
    "        self.device = device\n",
    "\n",
    "    def compute_loss(self, model, batch, return_outputs=False):\n",
    "\n",
    "        b_input_ids = batch[\"input_ids\"].to(self.device)\n",
    "        b_input_mask = batch[\"attention_mask\"].to(self.device)\n",
    "        b_labels = batch[\"labels\"].to(self.device)\n",
    "        decoder_attention_mask = batch[\"decoder_attention_mask\"].to(self.device)\n",
    "\n",
    "        outputs = model(\n",
    "            b_input_ids,\n",
    "            attention_mask=b_input_mask,\n",
    "            decoder_attention_mask=decoder_attention_mask,\n",
    "            labels=b_labels,\n",
    "        )\n",
    "        loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "args= RunArguments(\"trained_models/\",run_id=\"first_attempt_robert2gpt_\",max_seq_len=480,per_device_eval_batch_size=4,per_device_train_batch_size=4)\n",
    "training_arguments = get_model_trainer_arguments(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Initializing roberta-base as a decoder model. Cross attention layers are added to roberta-base and randomly initialized if roberta-base's architecture allows for cross attention layers.\n",
      "loading weights file pytorch_model.bin from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
      "\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Num Params: 277714524\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from transformers.trainer_callback import EarlyStoppingCallback\n",
    "\n",
    "custom_trainer = CustomTrainer(\n",
    "        model_init=partial(model_init,),\n",
    "        args=training_arguments,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=SmartCollator(pad_token_id=train_dataset.tokenizer.pad_token_id, \n",
    "                                    max_len=args.max_seq_len),  # type: ignore\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=6)],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of RobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaModel for predictions without further training.\n",
      "loading configuration file config.json from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "Initializing roberta-base as a decoder model. Cross attention layers are added to roberta-base and randomly initialized if roberta-base's architecture allows for cross attention layers.\n",
      "loading weights file pytorch_model.bin from cache at /home/azureuser/.cache/huggingface/hub/models--roberta-base/snapshots/ff46155979338ff8063cdad90908b498ab91b181/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForCausalLM.\n",
      "\n",
      "Some weights of RobertaForCausalLM were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.encoder.layer.1.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.8.crossattention.self.query.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.2.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.query.weight', 'roberta.encoder.layer.7.crossattention.self.query.weight', 'roberta.encoder.layer.4.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.dense.bias', 'roberta.encoder.layer.5.crossattention.self.key.bias', 'roberta.encoder.layer.5.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.output.dense.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.key.weight', 'roberta.encoder.layer.6.crossattention.output.dense.bias', 'roberta.encoder.layer.3.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.output.dense.weight', 'roberta.encoder.layer.6.crossattention.self.query.bias', 'roberta.encoder.layer.6.crossattention.self.value.weight', 'roberta.encoder.layer.5.crossattention.output.dense.weight', 'roberta.encoder.layer.4.crossattention.self.key.weight', 'roberta.encoder.layer.7.crossattention.self.value.bias', 'roberta.encoder.layer.5.crossattention.self.key.weight', 'roberta.encoder.layer.1.crossattention.self.key.bias', 'roberta.encoder.layer.2.crossattention.self.value.bias', 'roberta.encoder.layer.1.crossattention.self.query.bias', 'roberta.encoder.layer.3.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.self.key.bias', 'roberta.encoder.layer.7.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.output.dense.weight', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.dense.bias', 'roberta.encoder.layer.2.crossattention.output.dense.bias', 'roberta.encoder.layer.9.crossattention.self.key.weight', 'roberta.encoder.layer.4.crossattention.self.key.bias', 'roberta.encoder.layer.10.crossattention.self.value.weight', 'roberta.encoder.layer.4.crossattention.self.value.weight', 'roberta.encoder.layer.0.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.dense.bias', 'roberta.encoder.layer.6.crossattention.self.key.bias', 'roberta.encoder.layer.6.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.key.weight', 'roberta.encoder.layer.2.crossattention.self.query.bias', 'roberta.encoder.layer.7.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.3.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.self.value.bias', 'roberta.encoder.layer.2.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.10.crossattention.self.query.bias', 'roberta.encoder.layer.9.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.1.crossattention.output.dense.bias', 'roberta.encoder.layer.1.crossattention.self.key.weight', 'roberta.encoder.layer.3.crossattention.self.value.bias', 'roberta.encoder.layer.6.crossattention.output.dense.weight', 'roberta.encoder.layer.11.crossattention.self.query.bias', 'roberta.encoder.layer.1.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.2.crossattention.self.query.weight', 'roberta.encoder.layer.3.crossattention.self.query.weight', 'roberta.encoder.layer.6.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.6.crossattention.self.value.bias', 'roberta.encoder.layer.11.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.7.crossattention.self.query.bias', 'roberta.encoder.layer.5.crossattention.self.value.weight', 'roberta.encoder.layer.3.crossattention.self.key.bias', 'roberta.encoder.layer.11.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.query.weight', 'roberta.encoder.layer.0.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.key.bias', 'roberta.encoder.layer.8.crossattention.self.query.weight', 'roberta.encoder.layer.2.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.5.crossattention.output.dense.bias', 'roberta.encoder.layer.10.crossattention.self.query.weight', 'roberta.encoder.layer.11.crossattention.self.value.weight', 'roberta.encoder.layer.9.crossattention.output.dense.weight', 'roberta.encoder.layer.10.crossattention.self.key.weight', 'roberta.encoder.layer.9.crossattention.self.query.bias', 'roberta.encoder.layer.11.crossattention.self.value.bias', 'roberta.encoder.layer.7.crossattention.output.dense.weight', 'roberta.encoder.layer.5.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.11.crossattention.output.LayerNorm.weight', 'roberta.encoder.layer.0.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.bias', 'roberta.encoder.layer.8.crossattention.self.key.weight', 'roberta.encoder.layer.10.crossattention.self.key.bias', 'roberta.encoder.layer.3.crossattention.self.value.weight', 'roberta.encoder.layer.8.crossattention.self.value.weight', 'roberta.encoder.layer.11.crossattention.self.query.weight', 'roberta.encoder.layer.8.crossattention.self.key.bias', 'roberta.encoder.layer.0.crossattention.self.query.bias', 'roberta.encoder.layer.4.crossattention.self.value.bias', 'roberta.encoder.layer.9.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.3.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.8.crossattention.output.dense.weight', 'roberta.encoder.layer.1.crossattention.self.value.bias', 'roberta.encoder.layer.10.crossattention.output.dense.weight', 'roberta.encoder.layer.7.crossattention.output.dense.bias', 'roberta.encoder.layer.4.crossattention.output.LayerNorm.bias', 'roberta.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Num Params: 277714524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/vsme/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 21451\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 26815\n",
      "  Number of trainable parameters = 277714524\n",
      "/anaconda/envs/vsme/lib/python3.9/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:634: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (520 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "custom_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=90"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vsme",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Oct 13 2022, 21:15:33) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "651a0ea9c99ff4fc9d0cad7e667d2a622e86969049e30a30403341f776f67719"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
