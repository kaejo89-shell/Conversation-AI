{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38a32395",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]=\"true\"\n",
    "from transformers import BartForConditionalGeneration,BartTokenizer\n",
    "import torch\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", \n",
    "                      model=\"facebook/bart-large-cnn\",\n",
    "                      device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15df0aee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8afbbea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextSummary(context,max_len=5000):\n",
    "    return summarizer(context[:max_len], \n",
    "                      max_length=max_len//3 + 150, \n",
    "                      min_length=150, \n",
    "                      do_sample=False)[0]['summary_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95e4adf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0767592807d419fa38a9c395c452b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/16.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f415391950449eca7ac434093944c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/7.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b221e1918e4119839c9211c08c31bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/52.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multidoc2dial/multidoc2dial\n",
      "No config specified, defaulting to: multidoc2dial/multidoc2dial\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset multidoc2dial/multidoc2dial to /home/nlplab/.cache/huggingface/datasets/multidoc2dial/multidoc2dial/1.0.0/6e2a407c09eb478a5b80845fc04406bb9c9d4bea9f135dd7b8b7610a6b608d84...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf92ff0372e401c9faedd5cab0fb9e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/6.87M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c827fc7b64d24940859571d466b8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/4201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "499a02e87f0f491fadb40ef20c507dcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/21451 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48aae501894743f1a7b9b77bcc1ec00a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset multidoc2dial downloaded and prepared to /home/nlplab/.cache/huggingface/datasets/multidoc2dial/multidoc2dial/1.0.0/6e2a407c09eb478a5b80845fc04406bb9c9d4bea9f135dd7b8b7610a6b608d84. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No config specified, defaulting to: multidoc2dial/multidoc2dial\n",
      "Found cached dataset multidoc2dial (/home/nlplab/.cache/huggingface/datasets/multidoc2dial/multidoc2dial/1.0.0/6e2a407c09eb478a5b80845fc04406bb9c9d4bea9f135dd7b8b7610a6b608d84)\n",
      "No config specified, defaulting to: multidoc2dial/multidoc2dial\n",
      "Found cached dataset multidoc2dial (/home/nlplab/.cache/huggingface/datasets/multidoc2dial/multidoc2dial/1.0.0/6e2a407c09eb478a5b80845fc04406bb9c9d4bea9f135dd7b8b7610a6b608d84)\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset_builder,load_dataset\n",
    "convAI_data = load_dataset_builder('multidoc2dial')\n",
    "\n",
    "train_data= load_dataset('multidoc2dial', split='train', ignore_verifications=True)\n",
    "test_data= load_dataset('multidoc2dial', split='test', \n",
    "                                            ignore_verifications=True)\n",
    "dev_data= load_dataset('multidoc2dial', split='validation', \n",
    "                                            ignore_verifications=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7096da9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from mosestokenizer import MosesDetokenizer, MosesSentenceSplitter\n",
    "import tqdm\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "\n",
    "splitter = MosesSentenceSplitter(\"en\")\n",
    "detokenizer = MosesDetokenizer(\"en\")\n",
    "\n",
    "\n",
    "def normalize_whitespace(string):\n",
    "    return re.sub(r\"(\\s)\\1{1,}\", r\"\\1\", string)\n",
    "\n",
    "\n",
    "def cleanDocument(document):\n",
    "    document = re.sub(r\"\\[\\d+\\s?\\]\", \"\", document)\n",
    "    document = re.sub(r\"(\\d\\.\\s+|[a-z]\\)\\s+|â€¢\\s+|[A-Z]\\.\\s+|[IVX]+\\.\\s+)\", \"\", document)\n",
    "    document = normalize_whitespace(document.replace(\"\\n\", \"\")).strip()\n",
    "    return document\n",
    "\n",
    "\n",
    "def raw_checks(answer, context, n=3):\n",
    "    dat = context.strip().split(\"\\n\\n\")\n",
    "    if len(dat) == 1:\n",
    "        dat = dat[0].strip().split(\"\\n\")\n",
    "\n",
    "    if len(dat) < 4:\n",
    "        n = len(dat)\n",
    "    document_list = [\"\".join(s) for s in list(ngrams(dat, n))]\n",
    "    # print(document_list)\n",
    "    contains = [s.strip() for s in document_list if answer.strip() in s.strip()]\n",
    "    if len(contains) > 1:\n",
    "        contains = random.choice(contains)\n",
    "        return contains\n",
    "    else:\n",
    "        return contains[-1]\n",
    "\n",
    "\n",
    "def finalOverlap(answer_context, document):\n",
    "    def nltk_splitter_based():\n",
    "        context_sentences = sent_tokenize(document)\n",
    "        if len(context_sentences) < 5:\n",
    "            return [document]\n",
    "        document_list = [\" \".join(s) for s in list(ngrams(context_sentences, 5))]\n",
    "        context_sentences = [s.strip() for s in document_list if answer_context in s]\n",
    "        return context_sentences\n",
    "\n",
    "    def split_sentences():\n",
    "        context_sentences = splitter([document])\n",
    "        document_list = [\" \".join(s) for s in list(ngrams(context_sentences, 5))]\n",
    "        context_sentences = [s.strip() for s in document_list if answer_context in s]\n",
    "        if len(context_sentences) == 0:\n",
    "            context_sentences = nltk_splitter_based()\n",
    "\n",
    "        if len(context_sentences) > 1:\n",
    "            return random.choice(context_sentences)\n",
    "        else:\n",
    "            return context_sentences[-1]\n",
    "\n",
    "    return split_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f2bc1245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eeb3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from dataclasses_json import dataclass_json\n",
    "\n",
    "@dataclass_json\n",
    "@dataclass\n",
    "class Multidoc2DialData:\n",
    "    document: str\n",
    "    current_question: str\n",
    "    conv_history: str\n",
    "    answer_phrase: str\n",
    "    facts: str\n",
    "    utterance: str\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9410f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processDataset(data_pack):\n",
    "    not_present = []\n",
    "    processed_data = []\n",
    "    for idx in tqdm.tqdm(range(0,len(data_pack)),desc=\"Processing: \"):\n",
    "        dat= train_data[idx]\n",
    "        document_context = dat['context']\n",
    "        question, history= dat['question'].split('[SEP]')\n",
    "        answer_text = dat['answers']['text'][0]\n",
    "        answer_ids = dat['answers']['answer_start'][0]\n",
    "        \n",
    "        if dat['answers']['text'][0] not in document_context:\n",
    "            not_present.append(idx)\n",
    "        else:\n",
    "            doc_context =  raw_checks(answer_text,document_context)\n",
    "            document = cleanDocument(doc_context)\n",
    "            answer_context = cleanDocument(answer_text)\n",
    "            assert answer_context in document, \"Error: 1\"\n",
    "            context_sentences=finalOverlap(answer_context,document)\n",
    "            assert answer_context.strip() in context_sentences.strip(), \"Error: 2\"\n",
    "            conversation = Multidoc2DialData(document=cleanDocument(document_context),\n",
    "                                             conv_history=history,\n",
    "                                             utterance=dat['utterance'],\n",
    "                                             facts=context_sentences,\n",
    "                                             answer_phrase= answer_context.strip(),\n",
    "                                             current_question=question)\n",
    "            processed_data.append(conversation)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f966ee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21451/21451 [00:12<00:00, 1660.47it/s]\n",
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4094/4094 [00:02<00:00, 1772.69it/s]\n",
      "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4201/4201 [00:02<00:00, 1716.65it/s]\n"
     ]
    }
   ],
   "source": [
    "processed_train= processDataset(train_data)\n",
    "processed_test= processDataset(test_data)\n",
    "processed_dev= processDataset(dev_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "4199ce7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3a9d94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "875ebaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"processed_data/train_data.json\",\"w\") as train_file:\n",
    "    json.dump(Multidoc2DialData.schema().dump(processed_train, many=True,),train_file)\n",
    "with open(\"processed_data/test_data.json\",\"w\") as test_file:\n",
    "    json.dump(Multidoc2DialData.schema().dump(processed_test, many=True,),test_file)\n",
    "    \n",
    "with open(\"processed_data/dev_data.json\",\"w\") as dev_file:\n",
    "    json.dump(Multidoc2DialData.schema().dump(processed_dev, many=True,),dev_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('development')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "473435c5caf2da67d3d84349b3ab99ae605588908510e1f3cdf041055f6c21f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
